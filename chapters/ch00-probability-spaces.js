window.CHAPTERS = window.CHAPTERS || [];
window.CHAPTERS.push({
    id: 'ch00',
    number: 0,
    title: 'Probability Spaces & Random Variables',
    subtitle: 'Probability Spaces & Random Variables',
    sections: [
        // ===== Section 1: σ-Algebras & Probability Measures =====
        {
            id: 'ch00-sec01',
            title: 'σ-Algebras & Probability Measures',
            content: `
                <h2>σ-Algebras & Probability Measures σ-代数与概率测度</h2>

                <div class="env-block intuition">
                    <div class="env-title">Intuition</div>
                    <div class="env-body">
                        <p>The foundation of probability theory rests on answering this question: to which "events" can we assign probabilities? Not all subsets can be assigned probabilities consistently (as shown by the Vitali construction), so we need a <strong>σ-algebra</strong> (σ-代数) to precisely delineate the scope of "measurable events." Kolmogorov established the axiomatic framework of probability theory in 1933, grounding probability in measure theory (测度论).</p>
                    </div>
                </div>

                <div class="env-block definition">
                    <div class="env-title">Definition 0.1 (σ-Algebra)</div>
                    <div class="env-body">
                        <p>Let \\(\\Omega\\) be a nonempty set. A <strong>σ-algebra</strong> (σ-代数) on \\(\\Omega\\) is a collection \\(\\mathcal{F} \\subseteq 2^{\\Omega}\\) satisfying:</p>
                        <ol>
                            <li>\\(\\Omega \\in \\mathcal{F}\\);</li>
                            <li>If \\(A \\in \\mathcal{F}\\), then \\(A^c \\in \\mathcal{F}\\) (closed under complementation);</li>
                            <li>If \\(A_1, A_2, \\ldots \\in \\mathcal{F}\\), then \\(\\bigcup_{n=1}^{\\infty} A_n \\in \\mathcal{F}\\) (closed under countable unions).</li>
                        </ol>
                        <p>We call \\((\\Omega, \\mathcal{F})\\) a <strong>measurable space</strong> (可测空间).</p>
                    </div>
                </div>

                <div class="env-block remark">
                    <div class="env-title">Remark</div>
                    <div class="env-body">
                        <p>By De Morgan's laws, a σ-algebra is also closed under countable intersections. The smallest σ-algebra is \\(\\{\\emptyset, \\Omega\\}\\), and the largest is \\(2^{\\Omega}\\) (the power set of all subsets). On \\(\\mathbb{R}\\), the most commonly used is the <strong>Borel σ-algebra</strong> (Borel σ-代数) \\(\\mathcal{B}(\\mathbb{R})\\), generated by all open sets.</p>
                    </div>
                </div>

                <div class="env-block definition">
                    <div class="env-title">Definition 0.2 (Probability Measure)</div>
                    <div class="env-body">
                        <p>Let \\((\\Omega, \\mathcal{F})\\) be a measurable space. A mapping \\(P: \\mathcal{F} \\to [0,1]\\) is called a <strong>probability measure</strong> (概率测度) on \\(\\mathcal{F}\\) if:</p>
                        <ol>
                            <li>\\(P(\\Omega) = 1\\) (normalization);</li>
                            <li>For any pairwise disjoint sequence \\(A_1, A_2, \\ldots \\in \\mathcal{F}\\),
                            \\[P\\!\\left(\\bigcup_{n=1}^{\\infty} A_n\\right) = \\sum_{n=1}^{\\infty} P(A_n)\\]
                            (countable additivity).</li>
                        </ol>
                        <p>The triple \\((\\Omega, \\mathcal{F}, P)\\) is called a <strong>probability space</strong> (概率空间).</p>
                    </div>
                </div>

                <div class="env-block theorem">
                    <div class="env-title">Theorem 0.3 (Basic Properties of Probability Measures)</div>
                    <div class="env-body">
                        <p>Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and \\(A, B \\in \\mathcal{F}\\). Then:</p>
                        <ol>
                            <li>\\(P(\\emptyset) = 0\\);</li>
                            <li>\\(P(A^c) = 1 - P(A)\\);</li>
                            <li><strong>Monotonicity</strong>: if \\(A \\subseteq B\\), then \\(P(A) \\leq P(B)\\);</li>
                            <li><strong>Inclusion-exclusion</strong>: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\);</li>
                            <li><strong>Sub-additivity</strong> (Boole's inequality): \\(P\\!\\left(\\bigcup_{n=1}^{\\infty} A_n\\right) \\leq \\sum_{n=1}^{\\infty} P(A_n)\\);</li>
                            <li><strong>Continuity</strong>: if \\(A_n \\uparrow A\\) (monotone increasing), then \\(P(A_n) \\to P(A)\\).</li>
                        </ol>
                    </div>
                </div>

                <div class="env-block proof">
                    <div class="env-title">Proof (partial)</div>
                    <div class="env-body">
                        <p>(1) Take \\(A_1 = \\Omega, A_n = \\emptyset\\) (\\(n \\geq 2\\)). By countable additivity:</p>
                        \\[1 = P(\\Omega) = P(\\Omega) + \\sum_{n=2}^{\\infty} P(\\emptyset) = 1 + \\sum_{n=2}^{\\infty} P(\\emptyset),\\]
                        <p>hence \\(P(\\emptyset) = 0\\).</p>
                        <p>(2) \\(\\Omega = A \\cup A^c\\) with \\(A \\cap A^c = \\emptyset\\), so by countable additivity \\(1 = P(A) + P(A^c)\\).</p>
                        <p>(4) Decompose \\(A \\cup B\\) into a disjoint union: \\(A \\cup B = A \\cup (B \\setminus A)\\), where \\(B = (A \\cap B) \\cup (B \\setminus A)\\), so \\(P(B \\setminus A) = P(B) - P(A \\cap B)\\). Therefore \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\).</p>
                        <div class="qed">∎</div>
                    </div>
                </div>

                <div class="env-block example">
                    <div class="env-title">Example 0.4</div>
                    <div class="env-body">
                        <p>Roll a fair die: \\(\\Omega = \\{1,2,3,4,5,6\\}\\), \\(\\mathcal{F} = 2^{\\Omega}\\), \\(P(\\{\\omega\\}) = 1/6\\).</p>
                        <p>Let \\(A = \\{2,4,6\\}\\) (even numbers) and \\(B = \\{1,2,3\\}\\) (at most 3). Then:</p>
                        \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = \\frac{3}{6} + \\frac{3}{6} - \\frac{1}{6} = \\frac{5}{6}.\\]
                    </div>
                </div>

                <p>The interactive visualization below illustrates the intuitive meaning of the inclusion-exclusion principle (容斥原理) on a Venn diagram. Drag the sliders to adjust \\(P(A)\\) and \\(P(B)\\) and observe how \\(P(A \\cup B)\\) changes.</p>

                <div class="viz-placeholder" data-viz="venn-probability-viz"></div>
            `,
            visualizations: [
                {
                    id: 'venn-probability-viz',
                    title: 'Interactive: Inclusion-Exclusion Venn Diagram 容斥原理',
                    description: 'Drag sliders to adjust P(A), P(B), P(A∩B) and observe Venn diagram changes',
                    setup: function(container, controls) {
                        var viz = new VizEngine(container, {width: 560, height: 400, scale: 40});

                        var pA = 0.5;
                        var pB = 0.4;
                        var pAB = 0.15;

                        var sliderA = VizEngine.createSlider(controls, 'P(A)', 0, 1, pA, 0.01, function(v) {
                            pA = v;
                            if (pAB > Math.min(pA, pB)) { pAB = Math.min(pA, pB); sliderAB.value = pAB; }
                            draw();
                        });
                        var sliderB = VizEngine.createSlider(controls, 'P(B)', 0, 1, pB, 0.01, function(v) {
                            pB = v;
                            if (pAB > Math.min(pA, pB)) { pAB = Math.min(pA, pB); sliderAB.value = pAB; }
                            draw();
                        });
                        var sliderAB = VizEngine.createSlider(controls, 'P(A∩B)', 0, 0.5, pAB, 0.01, function(v) {
                            pAB = Math.min(v, Math.min(pA, pB));
                            draw();
                        });

                        function draw() {
                            viz.clear();

                            var ctx = viz.ctx;
                            var cx = viz.width / 2;
                            var cy = viz.height / 2 - 10;

                            var rA = Math.sqrt(pA) * 120;
                            var rB = Math.sqrt(pB) * 120;
                            var overlap = pAB > 0 ? Math.sqrt(pAB) * 80 : rA + rB + 10;
                            var dist = rA + rB - overlap;
                            if (dist < 0) dist = 0;

                            var xA = cx - dist / 2;
                            var xB = cx + dist / 2;

                            // Draw circle A
                            ctx.globalAlpha = 0.3;
                            ctx.fillStyle = viz.colors.blue;
                            ctx.beginPath();
                            ctx.arc(xA, cy, rA, 0, 2 * Math.PI);
                            ctx.fill();

                            // Draw circle B
                            ctx.fillStyle = viz.colors.orange;
                            ctx.beginPath();
                            ctx.arc(xB, cy, rB, 0, 2 * Math.PI);
                            ctx.fill();

                            ctx.globalAlpha = 1.0;

                            // Outlines
                            ctx.strokeStyle = viz.colors.blue;
                            ctx.lineWidth = 2;
                            ctx.beginPath();
                            ctx.arc(xA, cy, rA, 0, 2 * Math.PI);
                            ctx.stroke();

                            ctx.strokeStyle = viz.colors.orange;
                            ctx.beginPath();
                            ctx.arc(xB, cy, rB, 0, 2 * Math.PI);
                            ctx.stroke();

                            // Labels
                            ctx.fillStyle = viz.colors.blue;
                            ctx.font = 'bold 16px -apple-system, sans-serif';
                            ctx.textAlign = 'center';
                            ctx.fillText('A', xA - rA * 0.4, cy - rA * 0.3);

                            ctx.fillStyle = viz.colors.orange;
                            ctx.fillText('B', xB + rB * 0.4, cy - rB * 0.3);

                            // Omega rectangle
                            ctx.strokeStyle = viz.colors.text;
                            ctx.lineWidth = 1;
                            ctx.setLineDash([4, 4]);
                            ctx.strokeRect(cx - 230, cy - 150, 460, 300);
                            ctx.setLineDash([]);

                            ctx.fillStyle = viz.colors.text;
                            ctx.font = '14px -apple-system, sans-serif';
                            ctx.textAlign = 'left';
                            ctx.fillText('Ω', cx - 220, cy - 132);

                            // Computed values
                            var pAuB = pA + pB - pAB;
                            if (pAuB > 1) pAuB = 1;

                            // Display formulas
                            var y0 = viz.height - 55;
                            ctx.fillStyle = viz.colors.white;
                            ctx.font = '14px -apple-system, sans-serif';
                            ctx.textAlign = 'center';
                            ctx.fillText('P(A) = ' + pA.toFixed(2) + '    P(B) = ' + pB.toFixed(2) + '    P(A∩B) = ' + pAB.toFixed(2), cx, y0);

                            ctx.fillStyle = viz.colors.teal;
                            ctx.font = 'bold 15px -apple-system, sans-serif';
                            ctx.fillText('P(A∪B) = P(A) + P(B) - P(A∩B) = ' + pAuB.toFixed(2), cx, y0 + 25);
                        }

                        draw();
                        return viz;
                    }
                }
            ],
            exercises: [
                {
                    question: 'Prove that if \\(\\mathcal{F}\\) is a σ-algebra, then \\(\\emptyset \\in \\mathcal{F}\\), and \\(\\mathcal{F}\\) is closed under countable intersections.',
                    hint: 'Use \\(\\emptyset = \\Omega^c\\) and De Morgan\'s law \\(\\bigcap_n A_n = \\left(\\bigcup_n A_n^c\\right)^c\\).',
                    solution: 'By definition \\(\\Omega \\in \\mathcal{F}\\) and \\(\\mathcal{F}\\) is closed under complements, so \\(\\emptyset = \\Omega^c \\in \\mathcal{F}\\). If \\(A_n \\in \\mathcal{F}\\), then \\(A_n^c \\in \\mathcal{F}\\), \\(\\bigcup_n A_n^c \\in \\mathcal{F}\\), hence \\(\\bigcap_n A_n = \\left(\\bigcup_n A_n^c\\right)^c \\in \\mathcal{F}\\).'
                },
                {
                    question: 'Let \\(P(A) = 0.6\\), \\(P(B) = 0.5\\), \\(P(A \\cap B) = 0.2\\). Find \\(P(A^c \\cap B^c)\\).',
                    hint: 'Use De Morgan\'s law: \\(A^c \\cap B^c = (A \\cup B)^c\\).',
                    solution: '\\(P(A \\cup B) = 0.6 + 0.5 - 0.2 = 0.9\\), hence \\(P(A^c \\cap B^c) = P((A \\cup B)^c) = 1 - 0.9 = 0.1\\).'
                },
                {
                    question: 'Prove Boole\'s inequality (sub-additivity): for any sequence of events \\(A_1, A_2, \\ldots\\), \\(P\\!\\left(\\bigcup_{n=1}^{\\infty} A_n\\right) \\leq \\sum_{n=1}^{\\infty} P(A_n)\\).',
                    hint: 'Define \\(B_1 = A_1\\), \\(B_n = A_n \\setminus \\bigcup_{k=1}^{n-1} A_k\\). Then \\(\\{B_n\\}\\) are pairwise disjoint and \\(\\bigcup B_n = \\bigcup A_n\\).',
                    solution: 'Let \\(B_1 = A_1\\), \\(B_n = A_n \\setminus \\bigcup_{k=1}^{n-1} A_k\\). Then \\(\\{B_n\\}\\) are pairwise disjoint, \\(\\bigcup_n B_n = \\bigcup_n A_n\\), and \\(B_n \\subseteq A_n\\), so \\(P(B_n) \\leq P(A_n)\\). Therefore \\(P\\!\\left(\\bigcup_n A_n\\right) = \\sum_n P(B_n) \\leq \\sum_n P(A_n)\\).'
                }
            ]
        },

        // ===== Section 2: Conditional Probability & Independence =====
        {
            id: 'ch00-sec02',
            title: 'Conditional Probability & Independence',
            content: `
                <h2>Conditional Probability & Independence 条件概率与独立性</h2>

                <div class="env-block intuition">
                    <div class="env-title">Intuition</div>
                    <div class="env-body">
                        <p>The core idea of <strong>conditional probability</strong> (条件概率) is "updating information": once we learn that event \\(B\\) has occurred, how should our belief about event \\(A\\) be adjusted? From the frequentist perspective, we "shrink" the sample space to \\(B\\), focusing only on the proportion of times \\(A\\) also occurs when \\(B\\) occurs. <strong>Bayes' theorem</strong> (贝叶斯定理) then provides the method for inverse reasoning — computing the probability of a "cause" from an observed "effect."</p>
                    </div>
                </div>

                <div class="env-block definition">
                    <div class="env-title">Definition 0.5 (Conditional Probability)</div>
                    <div class="env-body">
                        <p>Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space, \\(B \\in \\mathcal{F}\\) with \\(P(B) > 0\\). The <strong>conditional probability</strong> (条件概率) of event \\(A \\in \\mathcal{F}\\) given \\(B\\) is defined as:</p>
                        \\[P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}.\\]
                    </div>
                </div>

                <div class="env-block theorem">
                    <div class="env-title">Theorem 0.6 (Law of Total Probability)</div>
                    <div class="env-body">
                        <p>Let \\(B_1, B_2, \\ldots, B_n\\) be a finite partition of \\(\\Omega\\) (i.e., pairwise disjoint with union equal to \\(\\Omega\\)), with each \\(P(B_i) > 0\\). Then for any \\(A \\in \\mathcal{F}\\):</p>
                        \\[P(A) = \\sum_{i=1}^{n} P(A \\mid B_i) \\, P(B_i).\\]
                    </div>
                </div>

                <div class="env-block proof">
                    <div class="env-title">Proof</div>
                    <div class="env-body">
                        <p>Since \\(A = A \\cap \\Omega = A \\cap \\left(\\bigcup_i B_i\\right) = \\bigcup_i (A \\cap B_i)\\), and \\(\\{A \\cap B_i\\}\\) are pairwise disjoint, we have</p>
                        \\[P(A) = \\sum_i P(A \\cap B_i) = \\sum_i P(A \\mid B_i) P(B_i).\\]
                        <div class="qed">∎</div>
                    </div>
                </div>

                <div class="env-block theorem">
                    <div class="env-title">Theorem 0.7 (Bayes' Theorem)</div>
                    <div class="env-body">
                        <p>Under the conditions of Theorem 0.6, for any \\(j\\), if \\(P(A) > 0\\), then:</p>
                        \\[P(B_j \\mid A) = \\frac{P(A \\mid B_j) \\, P(B_j)}{\\sum_{i=1}^{n} P(A \\mid B_i) \\, P(B_i)}.\\]
                        <p>Here \\(P(B_j)\\) is the <strong>prior probability</strong> (先验概率), \\(P(B_j \\mid A)\\) is the <strong>posterior probability</strong> (后验概率), and \\(P(A \\mid B_j)\\) is the <strong>likelihood</strong> (似然).</p>
                    </div>
                </div>

                <div class="env-block example">
                    <div class="env-title">Example 0.8 (Disease Testing)</div>
                    <div class="env-body">
                        <p>A rare disease has prevalence \\(p = 0.001\\). The test has sensitivity (true positive rate) of 0.99 and specificity (true negative rate) of 0.95. If a person tests positive, the probability of actually having the disease is:</p>
                        \\[P(D \\mid +) = \\frac{0.99 \\times 0.001}{0.99 \\times 0.001 + 0.05 \\times 0.999} \\approx 0.0194\\]
                        <p>where \\(D\\) denotes "has the disease."</p>
                        <p>Even though the test appears "very accurate," because the prior probability is extremely low, the positive predictive value (PPV) is only about 1.94%. This is the classic manifestation of the <strong>base rate fallacy</strong> (基础率谬误).</p>
                    </div>
                </div>

                <p>The interactive visualization below demonstrates Bayes' theorem applied to disease testing. Drag the sliders to adjust the prior prevalence, sensitivity, and specificity, and observe how the posterior probability changes.</p>

                <div class="viz-placeholder" data-viz="bayes-disease-viz"></div>

                <div class="env-block definition">
                    <div class="env-title">Definition 0.9 (Independence of Events)</div>
                    <div class="env-body">
                        <p>Events \\(A\\) and \\(B\\) are <strong>independent</strong> (独立) if \\(P(A \\cap B) = P(A) \\cdot P(B)\\).</p>
                        <p>A family of events \\(\\{A_i\\}_{i \\in I}\\) is <strong>mutually independent</strong> (相互独立) if for any finite subset \\(J \\subseteq I\\):</p>
                        \\[P\\!\\left(\\bigcap_{j \\in J} A_j\\right) = \\prod_{j \\in J} P(A_j).\\]
                    </div>
                </div>

                <div class="env-block warning">
                    <div class="env-title">Warning</div>
                    <div class="env-body">
                        <p><strong>Pairwise independence \\(\\neq\\) mutual independence.</strong> Classic counterexample: flip a fair coin independently twice. Let \\(A\\) = "first flip is heads," \\(B\\) = "second flip is heads," \\(C\\) = "both flips have the same result." One can verify that \\(A, B, C\\) are pairwise independent but not mutually independent, since \\(P(A \\cap B \\cap C) = 1/4 \\neq P(A) P(B) P(C) = 1/8\\).</p>
                    </div>
                </div>
            `,
            visualizations: [
                {
                    id: 'bayes-disease-viz',
                    title: 'Interactive: Bayesian Disease Testing 贝叶斯疾病检测',
                    description: 'Adjust prevalence, sensitivity, and specificity to observe posterior probability changes',
                    setup: function(container, controls) {
                        var viz = new VizEngine(container, {width: 560, height: 420, scale: 40});

                        var prevalence = 0.01;
                        var sensitivity = 0.99;
                        var specificity = 0.95;

                        VizEngine.createSlider(controls, 'Prevalence (发病率)', 0.001, 0.3, prevalence, 0.001, function(v) {
                            prevalence = v; draw();
                        });
                        VizEngine.createSlider(controls, 'Sensitivity (灵敏度)', 0.5, 1.0, sensitivity, 0.01, function(v) {
                            sensitivity = v; draw();
                        });
                        VizEngine.createSlider(controls, 'Specificity (特异度)', 0.5, 1.0, specificity, 0.01, function(v) {
                            specificity = v; draw();
                        });

                        function draw() {
                            viz.clear();
                            var ctx = viz.ctx;
                            var w = viz.width;
                            var h = viz.height;

                            // Compute Bayesian posterior
                            var pPosGivenD = sensitivity;
                            var pPosGivenNotD = 1 - specificity;
                            var pPos = pPosGivenD * prevalence + pPosGivenNotD * (1 - prevalence);
                            var ppv = (pPosGivenD * prevalence) / pPos;
                            var npv = (specificity * (1 - prevalence)) / (1 - pPos);

                            // Draw a natural frequency icon array (1000 people)
                            var N = 1000;
                            var nSick = Math.round(prevalence * N);
                            var nHealthy = N - nSick;
                            var nTruePos = Math.round(sensitivity * nSick);
                            var nFalseNeg = nSick - nTruePos;
                            var nFalsePos = Math.round((1 - specificity) * nHealthy);
                            var nTrueNeg = nHealthy - nFalsePos;

                            // Bar chart of the four groups
                            var barX = 60;
                            var barW = 90;
                            var gap = 30;
                            var maxH = 250;
                            var baseY = h - 80;

                            var groups = [
                                {label: 'TP', count: nTruePos, color: viz.colors.green},
                                {label: 'FP', count: nFalsePos, color: viz.colors.orange},
                                {label: 'FN', count: nFalseNeg, color: viz.colors.red},
                                {label: 'TN', count: nTrueNeg, color: viz.colors.blue}
                            ];

                            var maxCount = Math.max(nTruePos, nFalsePos, nFalseNeg, nTrueNeg, 1);

                            for (var i = 0; i < groups.length; i++) {
                                var g = groups[i];
                                var bx = barX + i * (barW + gap);
                                var bh = (g.count / maxCount) * maxH;

                                ctx.fillStyle = g.color + '88';
                                ctx.fillRect(bx, baseY - bh, barW, bh);
                                ctx.strokeStyle = g.color;
                                ctx.lineWidth = 2;
                                ctx.strokeRect(bx, baseY - bh, barW, bh);

                                ctx.fillStyle = viz.colors.white;
                                ctx.font = 'bold 14px -apple-system, sans-serif';
                                ctx.textAlign = 'center';
                                ctx.fillText(g.count.toString(), bx + barW / 2, baseY - bh - 10);

                                ctx.fillStyle = viz.colors.text;
                                ctx.font = '12px -apple-system, sans-serif';
                                ctx.fillText(g.label, bx + barW / 2, baseY + 15);
                            }

                            // Horizontal baseline
                            ctx.strokeStyle = viz.colors.text;
                            ctx.lineWidth = 1;
                            ctx.beginPath();
                            ctx.moveTo(barX - 10, baseY);
                            ctx.lineTo(barX + 4 * (barW + gap), baseY);
                            ctx.stroke();

                            // Title and results
                            ctx.fillStyle = viz.colors.white;
                            ctx.font = 'bold 16px -apple-system, sans-serif';
                            ctx.textAlign = 'center';
                            ctx.fillText('Bayes: N = ' + N + ' people', w / 2, 25);

                            ctx.fillStyle = viz.colors.teal;
                            ctx.font = 'bold 15px -apple-system, sans-serif';
                            ctx.fillText('PPV = P(D|+) = ' + ppv.toFixed(4) + ' (' + (ppv * 100).toFixed(1) + '%)', w / 2, 52);

                            ctx.fillStyle = viz.colors.purple;
                            ctx.font = '13px -apple-system, sans-serif';
                            ctx.fillText('NPV = P(healthy|−) = ' + npv.toFixed(4) + ' (' + (npv * 100).toFixed(1) + '%)', w / 2, 74);

                            // Legend
                            var legY = baseY + 38;
                            ctx.font = '11px -apple-system, sans-serif';
                            ctx.textAlign = 'left';
                            var legItems = [
                                {c: viz.colors.green, t: 'TP = True Positive'},
                                {c: viz.colors.orange, t: 'FP = False Positive'},
                                {c: viz.colors.red, t: 'FN = False Negative'},
                                {c: viz.colors.blue, t: 'TN = True Negative'}
                            ];
                            for (var j = 0; j < legItems.length; j++) {
                                ctx.fillStyle = legItems[j].c;
                                ctx.fillRect(30 + j * 140, legY, 10, 10);
                                ctx.fillStyle = viz.colors.text;
                                ctx.fillText(legItems[j].t, 44 + j * 140, legY + 9);
                            }
                        }

                        draw();
                        return viz;
                    }
                }
            ],
            exercises: [
                {
                    question: 'Let \\(P(A) = 0.3\\), \\(P(B) = 0.4\\), and \\(A, B\\) be independent. Find \\(P(A \\mid A \\cup B)\\).',
                    hint: 'First compute \\(P(A \\cap B) = P(A)P(B)\\) by independence, then find \\(P(A \\cup B)\\) by inclusion-exclusion. Note that \\(P(A \\cap (A \\cup B)) = P(A)\\).',
                    solution: '\\(P(A \\cup B) = 0.3 + 0.4 - 0.3 \\times 0.4 = 0.58\\). Since \\(A \\subseteq A \\cup B\\), we have \\(P(A \\cap (A \\cup B)) = P(A) = 0.3\\). Therefore \\(P(A \\mid A \\cup B) = 0.3 / 0.58 \\approx 0.5172\\).'
                },
                {
                    question: 'Using Bayes\' theorem, if the prior prevalence increases from 0.001 to 0.1 (other parameters unchanged: sensitivity 0.99, specificity 0.95), what is the positive predictive value (PPV)?',
                    hint: 'Substitute into the Bayes formula: \\(\\text{PPV} = \\frac{0.99 \\times 0.1}{0.99 \\times 0.1 + 0.05 \\times 0.9}\\).',
                    solution: '\\(\\text{PPV} = \\frac{0.099}{0.099 + 0.045} = \\frac{0.099}{0.144} \\approx 0.6875\\), i.e., about 68.75%. The large increase in prior probability leads to a significant increase in the posterior probability.'
                },
                {
                    question: 'Prove that if \\(A\\) and \\(B\\) are independent, then \\(A\\) and \\(B^c\\) are also independent.',
                    hint: 'Decompose \\(A\\) as \\(A = (A \\cap B) \\cup (A \\cap B^c)\\).',
                    solution: 'From \\(A = (A \\cap B) \\cup (A \\cap B^c)\\) and disjointness: \\(P(A) = P(A \\cap B) + P(A \\cap B^c)\\). Since \\(A, B\\) are independent: \\(P(A \\cap B) = P(A)P(B)\\). Hence \\(P(A \\cap B^c) = P(A) - P(A)P(B) = P(A)(1 - P(B)) = P(A)P(B^c)\\).'
                }
            ]
        },

        // ===== Section 3: Random Variables & Distribution Functions =====
        {
            id: 'ch00-sec03',
            title: 'Random Variables & Distribution Functions',
            content: `
                <h2>Random Variables & Distribution Functions 随机变量与分布函数</h2>

                <div class="env-block intuition">
                    <div class="env-title">Intuition</div>
                    <div class="env-body">
                        <p>In practice, we often care not about the abstract sample point \\(\\omega\\), but about numerical information extracted from an experiment — such as "the number shown on a die roll" or "the thermometer reading." A <strong>random variable</strong> (随机变量) is the bridge that maps the abstract probability space to the real line. It must be a <strong>measurable function</strong> (可测函数), ensuring that events like "\\(X \\leq x\\)" belong to the σ-algebra and can therefore be assigned probabilities.</p>
                    </div>
                </div>

                <div class="env-block definition">
                    <div class="env-title">Definition 0.10 (Random Variable)</div>
                    <div class="env-body">
                        <p>Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space. A mapping \\(X: \\Omega \\to \\mathbb{R}\\) is called a (real-valued) <strong>random variable</strong> (随机变量) if for every \\(x \\in \\mathbb{R}\\):</p>
                        \\[\\{\\omega \\in \\Omega : X(\\omega) \\leq x\\} \\in \\mathcal{F}.\\]
                        <p>That is, \\(X\\) is a measurable function from \\((\\Omega, \\mathcal{F})\\) to \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\).</p>
                    </div>
                </div>

                <div class="env-block definition">
                    <div class="env-title">Definition 0.11 (Cumulative Distribution Function)</div>
                    <div class="env-body">
                        <p>The <strong>cumulative distribution function</strong> (累积分布函数, CDF) of a random variable \\(X\\) is \\(F_X: \\mathbb{R} \\to [0,1]\\):</p>
                        \\[F_X(x) = P(X \\leq x).\\]
                    </div>
                </div>

                <div class="env-block theorem">
                    <div class="env-title">Theorem 0.12 (Properties of the CDF)</div>
                    <div class="env-body">
                        <p>Any CDF \\(F\\) satisfies:</p>
                        <ol>
                            <li>\\(F\\) is monotonically non-decreasing;</li>
                            <li>\\(F\\) is right-continuous: \\(\\lim_{x \\to a^+} F(x) = F(a)\\);</li>
                            <li>\\(\\lim_{x \\to -\\infty} F(x) = 0\\), \\(\\lim_{x \\to +\\infty} F(x) = 1\\).</li>
                        </ol>
                        <p>Conversely, any function satisfying these three properties is the CDF of some random variable.</p>
                    </div>
                </div>

                <div class="env-block definition">
                    <div class="env-title">Definition 0.13 (PMF & PDF)</div>
                    <div class="env-body">
                        <p><strong>Discrete case:</strong> If \\(X\\) takes values in a countable set \\(\\{x_1, x_2, \\ldots\\}\\), its <strong>probability mass function</strong> (概率质量函数, PMF) is \\(p_X(x_k) = P(X = x_k)\\), with \\(\\sum_k p_X(x_k) = 1\\). In this case the CDF is a step function.</p>
                        <p><strong>Continuous case:</strong> If there exists a non-negative integrable function \\(f_X\\) such that \\(F_X(x) = \\int_{-\\infty}^{x} f_X(t) \\, dt\\), then \\(f_X\\) is called the <strong>probability density function</strong> (概率密度函数, PDF) of \\(X\\). At points where \\(F\\) is differentiable, \\(f_X(x) = F_X'(x)\\).</p>
                    </div>
                </div>

                <div class="env-block example">
                    <div class="env-title">Example 0.14</div>
                    <div class="env-body">
                        <p><strong>Discrete:</strong> A Bernoulli(\\(p\\)) random variable takes values in \\(\\{0,1\\}\\), with PMF \\(p_X(1) = p\\), \\(p_X(0) = 1-p\\). Its CDF is:</p>
                        \\[F_X(x) = \\begin{cases} 0 & x < 0 \\\\ 1-p & 0 \\leq x < 1 \\\\ 1 & x \\geq 1 \\end{cases}\\]
                        <p><strong>Continuous:</strong> The exponential distribution \\(\\text{Exp}(\\lambda)\\) has PDF and CDF respectively:</p>
                        \\[f_X(x) = \\lambda e^{-\\lambda x} \\mathbf{1}_{x \\geq 0}, \\quad F_X(x) = (1 - e^{-\\lambda x}) \\mathbf{1}_{x \\geq 0}.\\]
                    </div>
                </div>

                <p>The visualization below lets you build a discrete CDF (step function) and compare it with a continuous CDF. Use the sliders to switch between distributions and observe the shape of the CDF.</p>

                <div class="viz-placeholder" data-viz="cdf-builder-viz"></div>
            `,
            visualizations: [
                {
                    id: 'cdf-builder-viz',
                    title: 'Interactive: CDF Comparison — Discrete vs Continuous 离散与连续',
                    description: 'Switch distribution types to compare step-function CDFs with smooth CDFs',
                    setup: function(container, controls) {
                        var viz = new VizEngine(container, {
                            width: 560, height: 400, scale: 50,
                            originX: 80, originY: 340
                        });

                        var mode = 'binomial';
                        var param1 = 10;
                        var param2 = 0.5;

                        VizEngine.createButton(controls, 'Binomial(n,p)', function() {
                            mode = 'binomial'; param1 = 10; param2 = 0.5;
                            sN.value = 10; sP.value = 0.5; draw();
                        });
                        VizEngine.createButton(controls, 'Normal(mu,sigma)', function() {
                            mode = 'normal'; param1 = 3; param2 = 1;
                            sN.value = 3; sP.value = 1; draw();
                        });
                        VizEngine.createButton(controls, 'Exponential(lambda)', function() {
                            mode = 'exponential'; param1 = 1; param2 = 0;
                            sN.value = 1; sP.value = 0; draw();
                        });

                        var sN = VizEngine.createSlider(controls, 'Param 1 (n / mu / lambda)', 0.1, 20, param1, 0.1, function(v) {
                            param1 = v; draw();
                        });
                        var sP = VizEngine.createSlider(controls, 'Param 2 (p / sigma / --)', 0.01, 5, param2, 0.01, function(v) {
                            param2 = v; draw();
                        });

                        function draw() {
                            viz.clear();
                            viz.drawGrid(1);
                            viz.drawAxes();

                            var ctx = viz.ctx;
                            var xMin = -1;
                            var xMax = 10;

                            if (mode === 'binomial') {
                                var n = Math.round(param1);
                                var p = Math.max(0.01, Math.min(0.99, param2));
                                xMin = -0.5;
                                xMax = n + 1;

                                // Draw CDF as step function
                                var cumP = 0;
                                viz.drawSegment(xMin, 0, 0, 0, viz.colors.blue, 2);
                                for (var k = 0; k <= n; k++) {
                                    var pmf = VizEngine.binomialPMF(k, n, p);
                                    var prevCum = cumP;
                                    cumP += pmf;
                                    // Horizontal line at cumP from k to k+1
                                    var right = (k === n) ? xMax : k + 1;
                                    viz.drawSegment(k, cumP, right, cumP, viz.colors.blue, 2);
                                    // Vertical jump
                                    viz.drawSegment(k, prevCum, k, cumP, viz.colors.blue, 1, true);
                                    // Filled circle at (k, cumP), open at (k, prevCum)
                                    viz.drawPoint(k, cumP, viz.colors.blue, null, 4);
                                    // Open circle
                                    if (k > 0 || prevCum > 0) {
                                        var sc = viz.toScreen(k, prevCum);
                                        ctx.strokeStyle = viz.colors.blue;
                                        ctx.lineWidth = 2;
                                        ctx.beginPath();
                                        ctx.arc(sc[0], sc[1], 4, 0, 2 * Math.PI);
                                        ctx.stroke();
                                    }
                                }

                                viz.screenText('Binomial(' + n + ', ' + p.toFixed(2) + ') CDF', viz.width / 2, 20, viz.colors.white, 15);
                            } else if (mode === 'normal') {
                                var mu = param1;
                                var sigma = Math.max(0.1, param2);
                                xMin = mu - 4 * sigma;
                                xMax = mu + 4 * sigma;

                                // Shade PDF (scaled)
                                viz.shadeUnder(function(x) { return VizEngine.normalPDF(x, mu, sigma) * 3; }, xMin, xMax, viz.colors.purple + '33');

                                // Draw CDF
                                viz.drawFunction(function(x) {
                                    return VizEngine.normalCDF(x, mu, sigma);
                                }, xMin, xMax, viz.colors.blue, 2.5);

                                // Draw PDF (scaled for visibility)
                                viz.drawFunction(function(x) {
                                    return VizEngine.normalPDF(x, mu, sigma) * 3;
                                }, xMin, xMax, viz.colors.purple, 1.5);

                                viz.screenText('Normal(' + mu.toFixed(1) + ', ' + sigma.toFixed(2) + ') — Blue: CDF, Purple: PDF (scaled)', viz.width / 2, 20, viz.colors.white, 13);
                            } else if (mode === 'exponential') {
                                var lam = Math.max(0.1, param1);
                                xMin = -0.5;
                                xMax = 6 / lam;

                                // Shade PDF
                                viz.shadeUnder(function(x) { return VizEngine.exponentialPDF(x, lam) * 2; }, 0, xMax, viz.colors.orange + '33');

                                // Draw CDF
                                viz.drawFunction(function(x) {
                                    return x < 0 ? 0 : 1 - Math.exp(-lam * x);
                                }, xMin, xMax, viz.colors.blue, 2.5);

                                // Draw PDF (scaled)
                                viz.drawFunction(function(x) {
                                    return VizEngine.exponentialPDF(x, lam) * 2;
                                }, xMin, xMax, viz.colors.orange, 1.5);

                                viz.screenText('Exp(' + lam.toFixed(1) + ') — Blue: CDF, Orange: PDF (scaled)', viz.width / 2, 20, viz.colors.white, 13);
                            }

                            // Reference lines
                            viz.drawSegment(xMin, 1, xMax, 1, viz.colors.text + '44', 1, true);
                        }

                        draw();
                        return viz;
                    }
                }
            ],
            exercises: [
                {
                    question: 'Let \\(X \\sim \\text{Uniform}(0, 1)\\). Find the distribution of \\(Y = -\\frac{1}{\\lambda} \\ln(1 - X)\\) where \\(\\lambda > 0\\).',
                    hint: 'Compute \\(P(Y \\leq y) = P\\left(-\\frac{1}{\\lambda} \\ln(1-X) \\leq y\\right)\\) using the uniform distribution of \\(X\\).',
                    solution: 'For \\(y > 0\\), \\(P(Y \\leq y) = P(X \\leq 1 - e^{-\\lambda y}) = 1 - e^{-\\lambda y}\\), so \\(Y \\sim \\text{Exp}(\\lambda)\\). This is the core principle of the inverse transform sampling method.'
                },
                {
                    question: 'Prove that if \\(F\\) is a CDF, then \\(P(a < X \\leq b) = F(b) - F(a)\\).',
                    hint: 'Decompose \\(\\{X \\leq b\\}\\) as \\(\\{X \\leq a\\} \\cup \\{a < X \\leq b\\}\\).',
                    solution: 'Since \\(\\{X \\leq b\\} = \\{X \\leq a\\} \\cup \\{a < X \\leq b\\}\\) is a disjoint union, \\(F(b) = P(X \\leq b) = P(X \\leq a) + P(a < X \\leq b) = F(a) + P(a < X \\leq b)\\).'
                },
                {
                    question: 'Let \\(X\\) be a discrete random variable taking values \\(\\{-1, 0, 2\\}\\) with probabilities \\(0.3, 0.5, 0.2\\) respectively. Write down its CDF and sketch it.',
                    hint: 'The CDF is right-continuous at jump points, and each jump height equals the probability at that point.',
                    solution: '\\(F(x) = 0\\) if \\(x < -1\\); \\(F(x) = 0.3\\) if \\(-1 \\leq x < 0\\); \\(F(x) = 0.8\\) if \\(0 \\leq x < 2\\); \\(F(x) = 1\\) if \\(x \\geq 2\\). This is a step function with three jumps.'
                }
            ]
        },

        // ===== Section 4: Expectation, Variance & Moments =====
        {
            id: 'ch00-sec04',
            title: 'Expectation, Variance & Moments',
            content: `
                <h2>Expectation, Variance & Moments 期望、方差与矩</h2>

                <div class="env-block intuition">
                    <div class="env-title">Intuition</div>
                    <div class="env-body">
                        <p>The <strong>expected value</strong> (期望值) can be understood as the "center of mass": if at each point \\(x\\) on the number line we place mass \\(P(X = x)\\) (discrete) or mass density \\(f(x)\\) (continuous), then \\(\\mathbb{E}[X]\\) is the fulcrum position that exactly balances this "lever." The <strong>variance</strong> (方差) \\(\\operatorname{Var}(X)\\) measures the dispersion of mass around the center — analogous to the moment of inertia in physics.</p>
                    </div>
                </div>

                <div class="env-block definition">
                    <div class="env-title">Definition 0.15 (Expectation)</div>
                    <div class="env-body">
                        <p>Let \\(X\\) be a random variable on the probability space \\((\\Omega, \\mathcal{F}, P)\\).</p>
                        <p><strong>Discrete case:</strong> If \\(\\sum_k |x_k| \\, p_X(x_k) < \\infty\\), define \\(\\mathbb{E}[X] = \\sum_k x_k \\, p_X(x_k)\\).</p>
                        <p><strong>Continuous case:</strong> If \\(\\int_{-\\infty}^{\\infty} |x| \\, f_X(x) \\, dx < \\infty\\), define \\(\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\, f_X(x) \\, dx\\).</p>
                        <p>In general (Lebesgue integral): \\(\\mathbb{E}[X] = \\int_{\\Omega} X \\, dP\\), provided the integral exists.</p>
                    </div>
                </div>

                <div class="env-block theorem">
                    <div class="env-title">Theorem 0.16 (LOTUS — Law of the Unconscious Statistician)</div>
                    <div class="env-body">
                        <p>Let \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be a Borel measurable function and \\(X\\) a random variable. Then:</p>
                        <p><strong>Discrete case:</strong> \\(\\mathbb{E}[g(X)] = \\sum_k g(x_k) \\, p_X(x_k)\\).</p>
                        <p><strong>Continuous case:</strong> \\(\\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) \\, f_X(x) \\, dx\\).</p>
                        <p>In other words, computing \\(\\mathbb{E}[g(X)]\\) does not require first finding the distribution of \\(g(X)\\).</p>
                    </div>
                </div>

                <div class="env-block definition">
                    <div class="env-title">Definition 0.17 (Variance & Standard Deviation)</div>
                    <div class="env-body">
                        <p>If \\(\\mathbb{E}[X^2] < \\infty\\), define:</p>
                        \\[\\operatorname{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2.\\]
                        <p>The <strong>standard deviation</strong> (标准差) is \\(\\operatorname{SD}(X) = \\sqrt{\\operatorname{Var}(X)}\\).</p>
                    </div>
                </div>

                <div class="env-block theorem">
                    <div class="env-title">Theorem 0.18 (Properties of Expectation & Variance)</div>
                    <div class="env-body">
                        <p>Suppose the expectations of \\(X, Y\\) exist and \\(a, b \\in \\mathbb{R}\\). Then:</p>
                        <ol>
                            <li><strong>Linearity</strong>: \\(\\mathbb{E}[aX + bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y]\\) (no independence required);</li>
                            <li>\\(\\operatorname{Var}(aX + b) = a^2 \\operatorname{Var}(X)\\);</li>
                            <li>If \\(X, Y\\) are independent, then \\(\\mathbb{E}[XY] = \\mathbb{E}[X] \\cdot \\mathbb{E}[Y]\\);</li>
                            <li>If \\(X, Y\\) are independent, then \\(\\operatorname{Var}(X + Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y)\\).</li>
                        </ol>
                    </div>
                </div>

                <div class="env-block definition">
                    <div class="env-title">Definition 0.19 (Moments & Central Moments)</div>
                    <div class="env-body">
                        <p>The \\(k\\)-th <strong>moment</strong> (矩) of a random variable \\(X\\) is \\(\\mu_k' = \\mathbb{E}[X^k]\\) (if it exists).</p>
                        <p>The \\(k\\)-th <strong>central moment</strong> (中心矩) is \\(\\mu_k = \\mathbb{E}[(X - \\mathbb{E}[X])^k]\\).</p>
                        <p>In particular: \\(\\mu_1' = \\mathbb{E}[X]\\) (mean), \\(\\mu_2 = \\operatorname{Var}(X)\\). The <strong>skewness</strong> (偏度) and <strong>kurtosis</strong> (峰度) are \\(\\gamma_1 = \\mu_3 / \\mu_2^{3/2}\\) and \\(\\kappa = \\mu_4 / \\mu_2^2 - 3\\) (excess kurtosis), respectively.</p>
                    </div>
                </div>

                <p>The visualization below models a probability distribution as a mass distribution on the number line. The fulcrum represents \\(\\mathbb{E}[X]\\); when the lever is perfectly balanced, the fulcrum position is the expected value. You can drag probability weights and observe how the "balance" tilts.</p>

                <div class="viz-placeholder" data-viz="balance-point-viz"></div>
            `,
            visualizations: [
                {
                    id: 'balance-point-viz',
                    title: 'Interactive: The "Balance Point" of Expectation 期望的平衡点',
                    description: 'Drag probability weights to observe how the expected value (fulcrum) changes',
                    setup: function(container, controls) {
                        var viz = new VizEngine(container, {
                            width: 560, height: 380, scale: 50,
                            originX: 80, originY: 280
                        });

                        // A discrete distribution on {1, 2, 3, 4, 5}
                        var probs = [0.1, 0.2, 0.4, 0.2, 0.1];
                        var values = [1, 2, 3, 4, 5];
                        var sliders = [];

                        function normalize() {
                            var total = probs.reduce(function(s, v) { return s + v; }, 0);
                            if (total > 0) {
                                for (var i = 0; i < probs.length; i++) {
                                    probs[i] = probs[i] / total;
                                }
                            }
                        }

                        for (var i = 0; i < values.length; i++) {
                            (function(idx) {
                                sliders.push(VizEngine.createSlider(controls, 'w(' + values[idx] + ')', 0, 1, probs[idx], 0.01, function(v) {
                                    probs[idx] = v;
                                    normalize();
                                    draw();
                                }));
                            })(i);
                        }

                        function draw() {
                            viz.clear();
                            var ctx = viz.ctx;

                            // Draw number line
                            var y0 = 3.5;
                            viz.drawSegment(0, y0, 6, y0, viz.colors.axis, 2);

                            // Tick marks and bars
                            for (var i = 0; i < values.length; i++) {
                                var x = values[i];
                                var p = probs[i];
                                var barHeight = p * 8; // scale for visibility

                                // Bar
                                var sc1 = viz.toScreen(x - 0.15, y0 + barHeight);
                                var sc2 = viz.toScreen(x + 0.15, y0);
                                ctx.fillStyle = viz.colors.blue + '99';
                                ctx.fillRect(sc1[0], sc1[1], sc2[0] - sc1[0], sc2[1] - sc1[1]);
                                ctx.strokeStyle = viz.colors.blue;
                                ctx.lineWidth = 1.5;
                                ctx.strokeRect(sc1[0], sc1[1], sc2[0] - sc1[0], sc2[1] - sc1[1]);

                                // Label value
                                viz.drawText(x.toString(), x, y0 - 0.5, viz.colors.white, 14);
                                // Label probability
                                viz.drawText(p.toFixed(2), x, y0 + barHeight + 0.4, viz.colors.teal, 12);
                            }

                            // Compute expectation and variance
                            var mu = 0;
                            for (var i = 0; i < values.length; i++) {
                                mu += values[i] * probs[i];
                            }
                            var variance = 0;
                            for (var i = 0; i < values.length; i++) {
                                variance += probs[i] * Math.pow(values[i] - mu, 2);
                            }

                            // Draw balance point (triangle)
                            var triSc = viz.toScreen(mu, y0);
                            ctx.fillStyle = viz.colors.orange;
                            ctx.beginPath();
                            ctx.moveTo(triSc[0], triSc[1]);
                            ctx.lineTo(triSc[0] - 10, triSc[1] + 20);
                            ctx.lineTo(triSc[0] + 10, triSc[1] + 20);
                            ctx.closePath();
                            ctx.fill();

                            // E[X] label
                            viz.screenText('E[X] = ' + mu.toFixed(3), triSc[0], triSc[1] + 35, viz.colors.orange, 14, 'center');

                            // Variance indicator: horizontal bar from mu-sd to mu+sd
                            var sd = Math.sqrt(variance);
                            viz.drawSegment(mu - sd, y0 - 0.7, mu + sd, y0 - 0.7, viz.colors.purple, 2);
                            viz.drawSegment(mu - sd, y0 - 0.5, mu - sd, y0 - 0.9, viz.colors.purple, 2);
                            viz.drawSegment(mu + sd, y0 - 0.5, mu + sd, y0 - 0.9, viz.colors.purple, 2);

                            viz.screenText('SD = ' + sd.toFixed(3) + ', Var = ' + variance.toFixed(3), viz.width / 2, 20, viz.colors.purple, 13);
                            viz.screenText('Drag sliders to adjust weights; they auto-normalize to sum 1', viz.width / 2, viz.height - 15, viz.colors.text, 11);
                        }

                        normalize();
                        draw();
                        return viz;
                    }
                }
            ],
            exercises: [
                {
                    question: 'Let \\(X \\sim \\text{Exp}(\\lambda)\\). Use LOTUS to compute \\(\\mathbb{E}[X]\\) and \\(\\operatorname{Var}(X)\\).',
                    hint: 'Use integration by parts: \\(\\mathbb{E}[X] = \\int_0^{\\infty} x \\lambda e^{-\\lambda x} dx\\). For the variance, first compute \\(\\mathbb{E}[X^2]\\).',
                    solution: '\\(\\mathbb{E}[X] = \\int_0^{\\infty} x \\lambda e^{-\\lambda x} dx = 1/\\lambda\\) (integration by parts). \\(\\mathbb{E}[X^2] = \\int_0^{\\infty} x^2 \\lambda e^{-\\lambda x} dx = 2/\\lambda^2\\) (two rounds of integration by parts). Hence \\(\\operatorname{Var}(X) = 2/\\lambda^2 - 1/\\lambda^2 = 1/\\lambda^2\\).'
                },
                {
                    question: 'Prove the variance shortcut formula: \\(\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\).',
                    hint: 'Expand \\(\\mathbb{E}[(X - \\mu)^2]\\) where \\(\\mu = \\mathbb{E}[X]\\).',
                    solution: '\\(\\operatorname{Var}(X) = \\mathbb{E}[(X - \\mu)^2] = \\mathbb{E}[X^2 - 2\\mu X + \\mu^2] = \\mathbb{E}[X^2] - 2\\mu \\mathbb{E}[X] + \\mu^2 = \\mathbb{E}[X^2] - 2\\mu^2 + \\mu^2 = \\mathbb{E}[X^2] - \\mu^2\\).'
                },
                {
                    question: 'Let \\(X_1, \\ldots, X_n\\) be i.i.d. with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\). Find \\(\\mathbb{E}[\\bar{X}]\\) and \\(\\operatorname{Var}(\\bar{X})\\).',
                    hint: 'Use linearity of expectation and the additivity of variance for independent random variables.',
                    solution: '\\(\\mathbb{E}[\\bar{X}] = \\frac{1}{n}\\sum \\mathbb{E}[X_i] = \\mu\\). \\(\\operatorname{Var}(\\bar{X}) = \\frac{1}{n^2}\\sum \\operatorname{Var}(X_i) = \\frac{\\sigma^2}{n}\\). This is the mathematical reason why the sample mean fluctuates less as the sample size increases.'
                }
            ]
        },

        // ===== Section 5: Moment Generating Functions & Characteristic Functions =====
        {
            id: 'ch00-sec05',
            title: 'Moment Generating Functions & Characteristic Functions',
            content: `
                <h2>Moment Generating Functions & Characteristic Functions 矩母函数与特征函数</h2>

                <div class="env-block intuition">
                    <div class="env-title">Intuition</div>
                    <div class="env-body">
                        <p>The <strong>moment generating function</strong> (矩母函数, MGF) and the <strong>characteristic function</strong> (特征函数, CF) are both tools that "encode" all the information about a random variable into a single function — similar to how the Fourier transform encodes a signal. The advantage of the MGF is that moments can be "extracted" simply by differentiation; the advantage of the characteristic function is that it always exists (unlike the MGF) and uniquely determines the distribution. These two tools are indispensable for proving core results such as the central limit theorem.</p>
                    </div>
                </div>

                <div class="env-block definition">
                    <div class="env-title">Definition 0.20 (Moment Generating Function)</div>
                    <div class="env-body">
                        <p>The <strong>moment generating function</strong> (矩母函数, MGF) of a random variable \\(X\\) is:</p>
                        \\[M_X(t) = \\mathbb{E}[e^{tX}], \\quad t \\in \\mathbb{R},\\]
                        <p>provided this expectation is finite on some open interval \\((-h, h)\\) containing 0.</p>
                    </div>
                </div>

                <div class="env-block theorem">
                    <div class="env-title">Theorem 0.21 (Relationship Between MGF & Moments)</div>
                    <div class="env-body">
                        <p>If \\(M_X(t)\\) exists on \\((-h, h)\\), then all moments of \\(X\\) exist, and:</p>
                        \\[\\mathbb{E}[X^k] = M_X^{(k)}(0) = \\left.\\frac{d^k}{dt^k} M_X(t) \\right|_{t=0}.\\]
                    </div>
                </div>

                <div class="env-block proof">
                    <div class="env-title">Proof</div>
                    <div class="env-body">
                        <p>Formally, expand \\(e^{tX}\\) as a Taylor series:</p>
                        \\[M_X(t) = \\mathbb{E}[e^{tX}] = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty} \\frac{(tX)^k}{k!}\\right] = \\sum_{k=0}^{\\infty} \\frac{\\mathbb{E}[X^k]}{k!} t^k,\\]
                        <p>where the interchange of expectation and summation is justified by the finiteness of \\(M_X(t)\\) on \\((-h,h)\\) (dominated convergence theorem). Differentiating \\(M_X(t)\\) \\(k\\) times with respect to \\(t\\) and setting \\(t=0\\) yields \\(\\mathbb{E}[X^k]\\).</p>
                        <div class="qed">∎</div>
                    </div>
                </div>

                <div class="env-block theorem">
                    <div class="env-title">Theorem 0.22 (Uniqueness Theorem for MGF)</div>
                    <div class="env-body">
                        <p>If the MGFs of two random variables \\(X, Y\\) exist and are equal on some open interval containing 0, i.e., \\(M_X(t) = M_Y(t)\\) for some \\(|t| < h\\), then \\(X\\) and \\(Y\\) have the same distribution.</p>
                    </div>
                </div>

                <div class="env-block example">
                    <div class="env-title">Example 0.23</div>
                    <div class="env-body">
                        <p><strong>MGF of the normal distribution \\(X \\sim N(\\mu, \\sigma^2)\\):</strong></p>
                        \\[M_X(t) = \\mathbb{E}[e^{tX}] = e^{\\mu t + \\sigma^2 t^2 / 2}.\\]
                        <p>Verification: \\(M_X'(t) = (\\mu + \\sigma^2 t) e^{\\mu t + \\sigma^2 t^2/2}\\), so \\(M_X'(0) = \\mu = \\mathbb{E}[X]\\).</p>
                        <p>\\(M_X''(t) = (\\sigma^2 + (\\mu + \\sigma^2 t)^2) e^{\\mu t + \\sigma^2 t^2/2}\\), so \\(M_X''(0) = \\sigma^2 + \\mu^2 = \\mathbb{E}[X^2]\\).</p>
                        <p>Therefore \\(\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\sigma^2\\).</p>
                    </div>
                </div>

                <div class="env-block definition">
                    <div class="env-title">Definition 0.24 (Characteristic Function)</div>
                    <div class="env-body">
                        <p>The <strong>characteristic function</strong> (特征函数) of a random variable \\(X\\) is:</p>
                        \\[\\varphi_X(t) = \\mathbb{E}[e^{itX}], \\quad t \\in \\mathbb{R}.\\]
                        <p>The characteristic function <strong>always exists and is bounded</strong> (\\(|\\varphi_X(t)| \\leq 1\\)), since \\(|e^{itX}| = 1\\). It is the Fourier transform of the distribution of \\(X\\) (with a conjugate convention) and uniquely determines the distribution of \\(X\\).</p>
                    </div>
                </div>

                <div class="env-block remark">
                    <div class="env-title">Remark</div>
                    <div class="env-body">
                        <p>The relationship between MGF and characteristic function: if the MGF exists, then \\(\\varphi_X(t) = M_X(it)\\). The key advantages of the characteristic function are:</p>
                        <ol>
                            <li>It exists for any random variable (the MGF may not exist, e.g., for the Cauchy distribution);</li>
                            <li>The characteristic function of a sum of independent random variables equals the product of their characteristic functions: \\(\\varphi_{X+Y}(t) = \\varphi_X(t) \\cdot \\varphi_Y(t)\\);</li>
                            <li>Levy's continuity theorem links pointwise convergence of characteristic functions to weak convergence of distributions — this is the key to proving the CLT.</li>
                        </ol>
                    </div>
                </div>

                <p>The visualization below demonstrates how differentiating the MGF extracts moments. You can select different distributions and observe the \\(M_X(t)\\) curve, as well as the tangent line slope at \\(t=0\\) (which equals \\(\\mathbb{E}[X]\\)) and the curvature (related to \\(\\mathbb{E}[X^2]\\)).</p>

                <div class="viz-placeholder" data-viz="mgf-derivative-viz"></div>
            `,
            visualizations: [
                {
                    id: 'mgf-derivative-viz',
                    title: 'Interactive: MGF & Moment Extraction 矩母函数与矩提取',
                    description: 'Select different distributions and observe how MGF derivatives at t=0 yield moments',
                    setup: function(container, controls) {
                        var viz = new VizEngine(container, {
                            width: 560, height: 400, scale: 40,
                            originX: 280, originY: 300
                        });

                        var dist = 'normal';
                        var mu = 1;
                        var sigma = 1;
                        var showTangent = true;

                        VizEngine.createButton(controls, 'Normal(mu,sigma)', function() {
                            dist = 'normal'; draw();
                        });
                        VizEngine.createButton(controls, 'Exponential(lambda)', function() {
                            dist = 'exponential'; draw();
                        });
                        VizEngine.createButton(controls, 'Poisson(lambda)', function() {
                            dist = 'poisson'; draw();
                        });

                        VizEngine.createSlider(controls, 'mu / lambda', 0.1, 5, mu, 0.1, function(v) {
                            mu = v; draw();
                        });
                        VizEngine.createSlider(controls, 'sigma (Normal only)', 0.1, 3, sigma, 0.1, function(v) {
                            sigma = v; draw();
                        });

                        function mgf(t) {
                            if (dist === 'normal') {
                                return Math.exp(mu * t + 0.5 * sigma * sigma * t * t);
                            } else if (dist === 'exponential') {
                                // MGF of Exp(lambda) = lambda / (lambda - t) for t < lambda
                                if (t >= mu) return NaN;
                                return mu / (mu - t);
                            } else if (dist === 'poisson') {
                                return Math.exp(mu * (Math.exp(t) - 1));
                            }
                            return 0;
                        }

                        function mgfPrime(t) {
                            // Numerical derivative
                            var dt = 0.0001;
                            return (mgf(t + dt) - mgf(t - dt)) / (2 * dt);
                        }

                        function mgfDoublePrime(t) {
                            var dt = 0.0001;
                            return (mgf(t + dt) - 2 * mgf(t) + mgf(t - dt)) / (dt * dt);
                        }

                        function draw() {
                            viz.clear();
                            viz.drawGrid(1);
                            viz.drawAxes();

                            var ctx = viz.ctx;

                            // Determine x range for t
                            var tMin = -3;
                            var tMax = 3;
                            if (dist === 'exponential') {
                                tMax = Math.min(mu - 0.1, 3);
                            }

                            // Draw MGF curve
                            viz.drawFunction(mgf, tMin, tMax, viz.colors.blue, 2.5, 300);

                            // Tangent line at t=0
                            var m0 = mgf(0); // should be 1
                            var slope = mgfPrime(0); // E[X]
                            var curve = mgfDoublePrime(0); // E[X^2]

                            if (showTangent && isFinite(slope)) {
                                // Tangent line: y = m0 + slope * t
                                viz.drawFunction(function(t) { return m0 + slope * t; }, tMin, tMax, viz.colors.orange, 1.5, 100);

                                // Mark the tangent point
                                viz.drawPoint(0, 1, viz.colors.orange, null, 6);
                            }

                            // Display moments
                            var ex = slope;
                            var ex2 = isFinite(curve) ? curve : NaN;
                            var varX = isFinite(ex2) ? ex2 - ex * ex : NaN;

                            viz.screenText('M(t) = E[e^tX]', viz.width / 2, 18, viz.colors.blue, 14);

                            var distLabel = '';
                            if (dist === 'normal') distLabel = 'N(' + mu.toFixed(1) + ', ' + sigma.toFixed(1) + '²)';
                            else if (dist === 'exponential') distLabel = 'Exp(' + mu.toFixed(1) + ')';
                            else if (dist === 'poisson') distLabel = 'Poi(' + mu.toFixed(1) + ')';

                            viz.screenText('Distribution: ' + distLabel, viz.width / 2, 38, viz.colors.teal, 13);

                            var line1 = "M'(0) = E[X] = " + (isFinite(ex) ? ex.toFixed(4) : 'N/A');
                            var line2 = "M''(0) = E[X²] = " + (isFinite(ex2) ? ex2.toFixed(4) : 'N/A');
                            var line3 = 'Var(X) = ' + (isFinite(varX) ? varX.toFixed(4) : 'N/A');

                            viz.screenText(line1, viz.width - 15, 60, viz.colors.orange, 13, 'right');
                            viz.screenText(line2, viz.width - 15, 78, viz.colors.purple, 13, 'right');
                            viz.screenText(line3, viz.width - 15, 96, viz.colors.green, 13, 'right');

                            // Legend
                            viz.screenText('Blue: M(t)    Orange: tangent at t=0 (slope = E[X])', viz.width / 2, viz.height - 12, viz.colors.text, 11);
                        }

                        draw();
                        return viz;
                    }
                }
            ],
            exercises: [
                {
                    question: 'Compute the MGF \\(M_X(t)\\) of a Poisson(\\(\\lambda\\)) random variable, and derive \\(\\mathbb{E}[X]\\) and \\(\\operatorname{Var}(X)\\) from it.',
                    hint: '\\(M_X(t) = \\mathbb{E}[e^{tX}] = \\sum_{k=0}^{\\infty} e^{tk} \\frac{\\lambda^k e^{-\\lambda}}{k!}\\). Combine \\(e^{tk} \\lambda^k\\) into \\((\\lambda e^t)^k\\).',
                    solution: '\\(M_X(t) = e^{-\\lambda} \\sum_{k=0}^{\\infty} \\frac{(\\lambda e^t)^k}{k!} = e^{-\\lambda} e^{\\lambda e^t} = e^{\\lambda(e^t - 1)}\\). \\(M_X\'(t) = \\lambda e^t \\cdot e^{\\lambda(e^t-1)}\\), so \\(M_X\'(0) = \\lambda = \\mathbb{E}[X]\\). \\(M_X\'\'(t) = (\\lambda e^t + \\lambda^2 e^{2t}) e^{\\lambda(e^t-1)}\\), so \\(M_X\'\'(0) = \\lambda + \\lambda^2 = \\mathbb{E}[X^2]\\). Therefore \\(\\operatorname{Var}(X) = \\lambda + \\lambda^2 - \\lambda^2 = \\lambda\\).'
                },
                {
                    question: 'Explain why the Cauchy distribution does not have an MGF but does have a characteristic function.',
                    hint: 'The tails of the Cauchy distribution decay like \\(1/x^2\\). Consider the integrability of \\(\\mathbb{E}[e^{tX}]\\) for \\(t \\neq 0\\).',
                    solution: 'The Cauchy distribution has PDF \\(f(x) = \\frac{1}{\\pi(1+x^2)}\\). For \\(t \\neq 0\\), \\(\\int |e^{tx}| f(x) dx = \\int \\frac{e^{tx}}{\\pi(1+x^2)} dx\\); as \\(x \\to +\\infty\\) (when \\(t > 0\\)), the integrand behaves like \\(e^{tx}/x^2 \\to \\infty\\), so the MGF does not exist. For the characteristic function \\(\\varphi(t) = \\mathbb{E}[e^{itX}]\\), we have \\(|e^{itX}| = 1\\), so the integral is always finite. In fact, \\(\\varphi_X(t) = e^{-|t|}\\).'
                },
                {
                    question: 'Let \\(X, Y\\) be independent with \\(X \\sim N(\\mu_1, \\sigma_1^2)\\), \\(Y \\sim N(\\mu_2, \\sigma_2^2)\\). Use the MGF to prove that \\(X + Y \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\\).',
                    hint: 'The MGF of a sum of independent random variables equals the product of their MGFs. Use the form of the normal MGF and the uniqueness theorem.',
                    solution: '\\(M_{X+Y}(t) = M_X(t) \\cdot M_Y(t) = e^{\\mu_1 t + \\sigma_1^2 t^2/2} \\cdot e^{\\mu_2 t + \\sigma_2^2 t^2/2} = e^{(\\mu_1+\\mu_2)t + (\\sigma_1^2+\\sigma_2^2)t^2/2}\\), which is the MGF of \\(N(\\mu_1+\\mu_2, \\sigma_1^2+\\sigma_2^2)\\). By the uniqueness theorem, \\(X+Y \\sim N(\\mu_1+\\mu_2, \\sigma_1^2+\\sigma_2^2)\\).'
                }
            ]
        }
    ]
});
